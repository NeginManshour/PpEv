{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251be020-a0a2-4ce3-9aa0-576786c5e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directories where the original Excel files are stored\n",
    "directories = {\n",
    "    'AlphaFold_Multimer_All_TB': 'path to /AlphaFold_Multimer_All_TB',\n",
    "    'Pyrosetta_DockQ_TB': 'path to /Pyrosetta_DockQ_TB',\n",
    "    'haddock_em_TB_DockQ': 'path to /haddock_em_TB_DockQ',\n",
    "    'haddock_md_TB_DockQ': 'path to /haddock_md_TB_DockQ',\n",
    "    'Deep_TB_DockQ': 'path to /Deep_DockQ_TB',\n",
    "    'Dove_TB_DockQ': 'path to /Dove_DockQ_TB',\n",
    "    'Vina_TB_DockQ': 'path to /vina_DockQ_TB',\n",
    "    'Vinardo_TB_DockQ': 'path to /vinardo_DockQ_TB',\n",
    "    'Foldx_S_TB_DockQ': 'path to /stability_TB',\n",
    "    'Foldx_int_TB_DockQ': 'path to /interaction_TB'\n",
    "}\n",
    "\n",
    "# Output directory to save the merged Excel files\n",
    "output_directory = 'path to /Heatmaps, Spearman_Correlation/Data/Merged_TB'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Specify which sheet to extract from each Excel file (if they are all named consistently as the first sheet, this could be simplified)\n",
    "sheet_names = {\n",
    "    'Sheet': 'AlphaFold_Multimer_All_TB',  # Assuming the default sheet name or modify as needed\n",
    "    'Pyrosetta': 'Pyrosetta_DockQ_TB',\n",
    "    'Hadd_em': 'haddock_em_TB_DockQ',\n",
    "    'Hadd_md': 'haddock_md_TB_DockQ',\n",
    "    'Deep_GNN': 'Deep_TB_DockQ',\n",
    "    'gnn_dove': 'Dove_TB_DockQ',\n",
    "    'Vina': 'Vina_TB_DockQ',\n",
    "    'Vinardo': 'Vinardo_TB_DockQ',\n",
    "    'Foldx': 'Foldx_S_TB_DockQ',\n",
    "    'Foldx_Int': 'Foldx_int_TB_DockQ'\n",
    "    \n",
    "}\n",
    "\n",
    "# Process each protein sample\n",
    "for file_name in os.listdir(directories['AlphaFold_Multimer_All_TB']):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        pdb_id = file_name.split('_')[0]\n",
    "        writer = pd.ExcelWriter(os.path.join(output_directory, f'{pdb_id}_merged.xlsx'), engine='xlsxwriter')\n",
    "        \n",
    "        for sheet, dir_key in sheet_names.items():\n",
    "            file_path = os.path.join(directories[dir_key], file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                data = pd.read_excel(file_path, sheet_name=sheet)  # Load data from the specified sheet\n",
    "                data.to_excel(writer, sheet_name=sheet, index=False)\n",
    "        \n",
    "        writer.save()\n",
    "\n",
    "print(\"All files have been processed and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60186be0-6beb-41cd-a105-1ff4fcf3bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directories where the original Excel files are stored\n",
    "directories = {\n",
    "    'AlphaFold_Multimer_All_TF': 'path to /AlphaFold_Multimer_All_TF',\n",
    "    'Pyrosetta_DockQ_TF': 'path to /Pyrosetta_DockQ_TF',\n",
    "    'haddock_em_TF_DockQ': 'path to /haddock_em_TF_DockQ',\n",
    "    'haddock_md_TF_DockQ': 'path to /haddock_md_TF_DockQ',\n",
    "    'Deep_TF_DockQ': 'path to /Deep_DockQ_TF',\n",
    "    'Dove_TF_DockQ': 'path to /Dove_DockQ_TF',\n",
    "    'Vina_TF_DockQ': 'path to /vina_DockQ_TF',\n",
    "    'Vinardo_TF_DockQ': 'path to /vinardo_DockQ_TF',\n",
    "    'Foldx_S_TF_DockQ': 'path to /stability_TF',\n",
    "    'Foldx_int_TF_DockQ': 'path to /interaction_TF'\n",
    "}\n",
    "\n",
    "# Output directory to save the merged Excel files\n",
    "output_directory = 'path to /Heatmaps, Spearman_Correlation/Data/Merged_TF'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Specify which sheet to extract from each Excel file (if they are all named consistently as the first sheet, this could be simplified)\n",
    "sheet_names = {\n",
    "    'Sheet': 'AlphaFold_Multimer_All_TF',  # Assuming the default sheet name or modify as needed\n",
    "    'Pyrosetta': 'Pyrosetta_DockQ_TF',\n",
    "    'Hadd_em': 'haddock_em_TF_DockQ',\n",
    "    'Hadd_md': 'haddock_md_TF_DockQ',\n",
    "    'Deep_GNN': 'Deep_TF_DockQ',\n",
    "    'gnn_dove': 'Dove_TF_DockQ',\n",
    "    'Vina': 'Vina_TF_DockQ',\n",
    "    'Vinardo': 'Vinardo_TF_DockQ',\n",
    "    'Foldx': 'Foldx_S_TF_DockQ',\n",
    "    'Foldx_Int': 'Foldx_int_TF_DockQ'\n",
    "}\n",
    "\n",
    "# Process each protein sample\n",
    "for file_name in os.listdir(directories['AlphaFold_Multimer_All_TF']):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        pdb_id = file_name.split('_')[0]\n",
    "        writer = pd.ExcelWriter(os.path.join(output_directory, f'{pdb_id}_merged.xlsx'), engine='xlsxwriter')\n",
    "        \n",
    "        for sheet, dir_key in sheet_names.items():\n",
    "            file_path = os.path.join(directories[dir_key], file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                data = pd.read_excel(file_path, sheet_name=sheet)  # Load data from the specified sheet\n",
    "                data.to_excel(writer, sheet_name=sheet, index=False)\n",
    "        \n",
    "        writer.save()\n",
    "\n",
    "print(\"All files have been processed and saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4a188-e8b9-4339-b07e-f20444a9c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_excel_files(input_directory, output_directory):\n",
    "    error_files = []  # List to store names of problematic files\n",
    "\n",
    "    # Loop through all Excel files in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".xlsx\"):\n",
    "            # Extract PDB ID from the filename (assuming it's before the first underscore)\n",
    "            pdb_id = filename.split('_')[0]\n",
    "            input_path = os.path.join(input_directory, filename)\n",
    "            output_path = os.path.join(output_directory, f\"{pdb_id}_Ranked_TB.xlsx\")\n",
    "            \n",
    "            try:\n",
    "                # Process each file\n",
    "                create_combined_excel(input_path, output_path)\n",
    "                #print(f\"Processed {filename} into {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                error_files.append(filename)\n",
    "    \n",
    "    # Print out the problematic files\n",
    "    if error_files:\n",
    "        print(\"Files that had issues:\")\n",
    "        for file in error_files:\n",
    "            print(file)\n",
    "\n",
    "def create_combined_excel(input_path, output_path):\n",
    "    try:\n",
    "        # Load data from different sheets\n",
    "        data = pd.read_excel(input_path, sheet_name='Pyrosetta', engine='openpyxl')\n",
    "        foldx_data = pd.read_excel(input_path, sheet_name='Foldx', engine='openpyxl')\n",
    "        foldx_int_data = pd.read_excel(input_path, sheet_name='Foldx_Int', engine='openpyxl')\n",
    "        hadd_em_data = pd.read_excel(input_path, sheet_name='Hadd_em', engine='openpyxl')\n",
    "        hadd_md_data = pd.read_excel(input_path, sheet_name='Hadd_md', engine='openpyxl')\n",
    "        \n",
    "        # Optional sheets\n",
    "        try:\n",
    "            vina_data = pd.read_excel(input_path, sheet_name='Vina', engine='openpyxl')\n",
    "            vina_rank = vina_data[\"alphafold rank\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Vina sheet not found in {input_path}: {e}\")\n",
    "            vina_rank = None\n",
    "        try:\n",
    "            vinardo_data = pd.read_excel(input_path, sheet_name='Vinardo', engine='openpyxl')\n",
    "            vinardo_rank = vinardo_data[\"alphafold rank\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Vinardo sheet not found in {input_path}: {e}\")\n",
    "            vinardo_rank = None\n",
    "        try:\n",
    "            gnn_dove_data = pd.read_excel(input_path, sheet_name='gnn_dove', engine='openpyxl')\n",
    "            gnn_dove_rank = gnn_dove_data[\"AlphaFold Rank\"]\n",
    "        except Exception as e:\n",
    "            print(f\"GNN Dove sheet not found in {input_path}: {e}\")\n",
    "            gnn_dove_rank = None\n",
    "        try:\n",
    "            deep_gnn_data = pd.read_excel(input_path, sheet_name='Deep_GNN', engine='openpyxl')\n",
    "            deep_gnn_rank = deep_gnn_data[\"AlphaRank\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Deep GNN sheet not found in {input_path}: {e}\")\n",
    "            deep_gnn_rank = None\n",
    "\n",
    "        # Rename and select columns, make a copy to avoid SettingWithCopyWarning\n",
    "        modified_data = data[[\"Pyrosetta_rank\", \"alphafold_rank\"]].copy()\n",
    "        modified_data.rename(columns={\"Pyrosetta_rank\": \"AlphaFold Rank\", \"alphafold_rank\": \"Pyrosetta Rank\"}, inplace=True)\n",
    "\n",
    "        # Add and rename columns from other sheets\n",
    "        modified_data[\"Fold_S Rank\"] = foldx_data[\"alphafold rank\"]\n",
    "        modified_data[\"Foldx_Int Rank\"] = foldx_int_data[\"alphafold rank\"]\n",
    "        modified_data[\"Hadd_em Rank\"] = hadd_em_data[\"old_rank\"]\n",
    "        modified_data[\"Hadd_md Rank\"] = hadd_md_data[\"old_rank\"]\n",
    "        if vina_rank is not None:\n",
    "            modified_data[\"Vina Rank\"] = vina_rank\n",
    "        if vinardo_rank is not None:\n",
    "            modified_data[\"Vinardo Rank\"] = vinardo_rank\n",
    "        if gnn_dove_rank is not None:\n",
    "            modified_data[\"GNN_Dove Rank\"] = gnn_dove_rank\n",
    "        if deep_gnn_rank is not None:\n",
    "            modified_data[\"Deep_GNN Rank\"] = deep_gnn_rank\n",
    "\n",
    "        # Save the final DataFrame to a new Excel file\n",
    "        modified_data.to_excel(output_path, index=False)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to process {input_path}: {str(e)}\")\n",
    "\n",
    "# Usage example:\n",
    "# Replace the paths with the actual paths you intend to use\n",
    "process_excel_files('path to/Merged_TB', 'output to/Merged/Rank_scoring_TB_vina')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b95ab-61bd-4af3-9033-931058eb23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_excel_files(input_directory, output_directory):\n",
    "    error_files = []  # List to store names of problematic files\n",
    "\n",
    "    # Loop through all Excel files in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".xlsx\"):\n",
    "            # Extract PDB ID from the filename (assuming it's before the first underscore)\n",
    "            pdb_id = filename.split('_')[0]\n",
    "            input_path = os.path.join(input_directory, filename)\n",
    "            output_path = os.path.join(output_directory, f\"{pdb_id}_Ranked_TF.xlsx\")\n",
    "            \n",
    "            try:\n",
    "                # Process each file\n",
    "                create_combined_excel(input_path, output_path)\n",
    "                #print(f\"Processed {filename} into {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                error_files.append(filename)\n",
    "    \n",
    "    # Print out the problematic files\n",
    "    if error_files:\n",
    "        print(\"Files that had issues:\")\n",
    "        for file in error_files:\n",
    "            print(file)\n",
    "\n",
    "def create_combined_excel(input_path, output_path):\n",
    "    try:\n",
    "        # Load data from different sheets\n",
    "        data = pd.read_excel(input_path, sheet_name='Pyrosetta', engine='openpyxl')\n",
    "        foldx_data = pd.read_excel(input_path, sheet_name='Foldx', engine='openpyxl')\n",
    "        foldx_int_data = pd.read_excel(input_path, sheet_name='Foldx_Int', engine='openpyxl')\n",
    "        hadd_em_data = pd.read_excel(input_path, sheet_name='Hadd_em', engine='openpyxl')\n",
    "        hadd_md_data = pd.read_excel(input_path, sheet_name='Hadd_md', engine='openpyxl')\n",
    "        \n",
    "        # Optional sheets\n",
    "        try:\n",
    "            vina_data = pd.read_excel(input_path, sheet_name='Vina', engine='openpyxl')\n",
    "            vina_rank = vina_data[\"alphafold rank\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Vina sheet not found in {input_path}: {e}\")\n",
    "            vina_rank = None\n",
    "        try:\n",
    "            vinardo_data = pd.read_excel(input_path, sheet_name='Vinardo', engine='openpyxl')\n",
    "            vinardo_rank = vinardo_data[\"alphafold rank\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Vinardo sheet not found in {input_path}: {e}\")\n",
    "            vinardo_rank = None\n",
    "        try:\n",
    "            gnn_dove_data = pd.read_excel(input_path, sheet_name='gnn_dove', engine='openpyxl')\n",
    "            gnn_dove_rank = gnn_dove_data[\"AlphaFold Rank\"]\n",
    "        except Exception as e:\n",
    "            print(f\"GNN Dove sheet not found in {input_path}: {e}\")\n",
    "            gnn_dove_rank = None\n",
    "        try:\n",
    "            deep_gnn_data = pd.read_excel(input_path, sheet_name='Deep_GNN', engine='openpyxl')\n",
    "            deep_gnn_rank = deep_gnn_data[\"AlphaRank\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Deep GNN sheet not found in {input_path}: {e}\")\n",
    "            deep_gnn_rank = None\n",
    "\n",
    "        # Rename and select columns, make a copy to avoid SettingWithCopyWarning\n",
    "        modified_data = data[[\"Pyrosetta_rank\", \"alphafold_rank\"]].copy()\n",
    "        modified_data.rename(columns={\"Pyrosetta_rank\": \"AlphaFold Rank\", \"alphafold_rank\": \"Pyrosetta Rank\"}, inplace=True)\n",
    "\n",
    "        # Add and rename columns from other sheets\n",
    "        modified_data[\"Fold_S Rank\"] = foldx_data[\"alphafold rank\"]\n",
    "        modified_data[\"Foldx_Int Rank\"] = foldx_int_data[\"alphafold rank\"]\n",
    "        modified_data[\"Hadd_em Rank\"] = hadd_em_data[\"old_rank\"]\n",
    "        modified_data[\"Hadd_md Rank\"] = hadd_md_data[\"old_rank\"]\n",
    "        if vina_rank is not None:\n",
    "            modified_data[\"Vina Rank\"] = vina_rank\n",
    "        if vinardo_rank is not None:\n",
    "            modified_data[\"Vinardo Rank\"] = vinardo_rank\n",
    "        if gnn_dove_rank is not None:\n",
    "            modified_data[\"GNN_Dove Rank\"] = gnn_dove_rank\n",
    "        if deep_gnn_rank is not None:\n",
    "            modified_data[\"Deep_GNN Rank\"] = deep_gnn_rank\n",
    "\n",
    "        # Save the final DataFrame to a new Excel file\n",
    "        modified_data.to_excel(output_path, index=False)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to process {input_path}: {str(e)}\")\n",
    "\n",
    "# Usage example:\n",
    "# Replace the paths with the actual paths you intend to use\n",
    "process_excel_files('path to/Merged/Merged_TF', 'output to/Merged/Rank_scoring_TF_vina')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0561a6f-e40c-438d-bb14-fb12401f03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process and modify Excel files\n",
    "def add_tb_to_numbers(input_dir, output_dir):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over all files in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            df = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "            # Add '_TB' to the end of all numbers in the DataFrame\n",
    "            for sheet_name, sheet_data in df.items():\n",
    "                df[sheet_name] = sheet_data.applymap(lambda x: f\"{x}_TB\" if isinstance(x, (int, float)) else x)\n",
    "\n",
    "            # Save the modified DataFrame to a new Excel file in the output directory\n",
    "            modified_file_path = os.path.join(output_dir, filename)\n",
    "            with pd.ExcelWriter(modified_file_path) as writer:\n",
    "                for sheet_name, sheet_data in df.items():\n",
    "                    sheet_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Define your input and output directories\n",
    "input_directory = 'path to/Merged/Rank_scoring_TB_vina'\n",
    "output_directory = 'output to/Merged/Rank_scoring_TB_TB_md'\n",
    "\n",
    "# Process the files\n",
    "add_tb_to_numbers(input_directory, output_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c59031-975f-4f1d-8352-319c8de94fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process and modify Excel files\n",
    "def add_tf_to_numbers(input_dir, output_dir):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over all files in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            df = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "            # Add '_TF' to the end of all numbers in the DataFrame\n",
    "            for sheet_name, sheet_data in df.items():\n",
    "                df[sheet_name] = sheet_data.applymap(lambda x: f\"{x}_TF\" if isinstance(x, (int, float)) else x)\n",
    "\n",
    "            # Save the modified DataFrame to a new Excel file in the output directory\n",
    "            modified_file_path = os.path.join(output_dir, filename)\n",
    "            with pd.ExcelWriter(modified_file_path) as writer:\n",
    "                for sheet_name, sheet_data in df.items():\n",
    "                    sheet_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Define your input and output directories\n",
    "input_directory = 'path to/Merged/Rank_scoring_TF_vina'\n",
    "output_directory = 'output to/Merged/Rank_scoring_TF_TF_md'\n",
    "\n",
    "# Process the files\n",
    "add_tf_to_numbers(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d981017-3877-4e25-8df3-f48ef44ee046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def update_cell_name(cell_value, pdb_id):\n",
    "    if isinstance(cell_value, str) and cell_value.endswith('_TB'):\n",
    "        base_value = cell_value[:-3]\n",
    "        return f\"{base_value}_{pdb_id}_TB\"\n",
    "    return cell_value\n",
    "\n",
    "def update_excel_files(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.xlsx'):\n",
    "            # Extract pdb_id from the filename (assuming it's the part before the first underscore or period)\n",
    "            pdb_id = filename.split('_')[0]\n",
    "            \n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            df = pd.read_excel(file_path)\n",
    "            df_updated = df.applymap(lambda x: update_cell_name(x, pdb_id))\n",
    "            \n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    "            df_updated.to_excel(output_file_path, index=False)\n",
    "            #print(f\"Updated file saved: {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_directory = 'path to/Merged/Rank_scoring_TB_TB_md'  # Replace with the path to your input directory\n",
    "output_directory = 'output to/Merged/Rank_scoring_TB_TB_pdb_md'  # Replace with the path to your output directory\n",
    "\n",
    "update_excel_files(input_directory, output_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309de2a-50c4-4938-ac57-66e64f634bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def update_cell_name(cell_value, pdb_id):\n",
    "    if isinstance(cell_value, str) and cell_value.endswith('_TF'):\n",
    "        base_value = cell_value[:-3]\n",
    "        return f\"{base_value}_{pdb_id}_TF\"\n",
    "    return cell_value\n",
    "\n",
    "def update_excel_files(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.xlsx'):\n",
    "            # Extract pdb_id from the filename (assuming it's the part before the first underscore or period)\n",
    "            pdb_id = filename.split('_')[0]\n",
    "            \n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            df = pd.read_excel(file_path)\n",
    "            df_updated = df.applymap(lambda x: update_cell_name(x, pdb_id))\n",
    "            \n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    "            df_updated.to_excel(output_file_path, index=False)\n",
    "            #print(f\"Updated file saved: {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_directory = 'path to/Merged/Rank_scoring_TF_TF_md'  # Replace with the path to your input directory\n",
    "output_directory = 'output to/Merged/Rank_scoring_TF_TF_pdb_md'  # Replace with the path to your output directory\n",
    "\n",
    "update_excel_files(input_directory, output_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353322e3-99c6-461d-8fbf-aa3d5a455962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the Excel files and the output directory\n",
    "input_directory = 'path to/Merged/Rank_scoring_TB_TB_pdb_md'\n",
    "output_directory = 'output to/Merged/Rank_scoring_TB_TB_pdb_three_md'\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['AlphaFold Rank', 'Fold_S Rank', 'Hadd_md Rank']\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each Excel file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        # Construct the full file path for input and output\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "        output_file_path = os.path.join(output_directory, filename)\n",
    "        \n",
    "        # Load the Excel file\n",
    "        df = pd.read_excel(input_file_path)\n",
    "        \n",
    "        # Keep only the specified columns\n",
    "        filtered_df = df[columns_to_keep]\n",
    "        \n",
    "        # Save the filtered DataFrame to a new Excel file in the output directory\n",
    "        filtered_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "        #print(f'Processed {filename} and saved to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c950d-8c3e-484e-9fd3-cfcb60b096d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the Excel files and the output directory\n",
    "input_directory = 'path to/Merged/Rank_scoring_TF_TF_pdb_md'\n",
    "output_directory = 'output to/Merged/Rank_scoring_TF_TF_pdb_three_md'\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['AlphaFold Rank', 'Fold_S Rank', 'Hadd_md Rank']\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each Excel file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        # Construct the full file path for input and output\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "        output_file_path = os.path.join(output_directory, filename)\n",
    "        \n",
    "        # Load the Excel file\n",
    "        df = pd.read_excel(input_file_path)\n",
    "        \n",
    "        # Keep only the specified columns\n",
    "        filtered_df = df[columns_to_keep]\n",
    "        \n",
    "        # Save the filtered DataFrame to a new Excel file in the output directory\n",
    "        filtered_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "        #print(f'Processed {filename} and saved to {output_file_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b1f3b-8a28-4dd1-a7f7-d0d99991ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directories\n",
    "tb_dir = 'path to/Merged/Rank_scoring_TB_TB_pdb_three_md'\n",
    "tf_dir = 'path to/Merged/Rank_scoring_TF_TF_pdb_three_md'\n",
    "output_dir = 'output to/Merged/combined_TB_TF_md'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extract the numerical parts and pdb_id from the rank column\n",
    "def extract_rank_and_pdb_id(rank):\n",
    "    \"\"\"Extracts the numerical part and pdb_id of the rank ignoring the '_TB' or '_TF' suffix.\"\"\"\n",
    "    if pd.isna(rank):\n",
    "        return None, None\n",
    "    parts = rank.split('_')\n",
    "    try:\n",
    "        # Convert the rank part to an integer (handles float conversion)\n",
    "        rank_number = int(float(parts[0]))\n",
    "    except ValueError:\n",
    "        return None, None  # Return a flag value if conversion fails\n",
    "    return rank_number, parts[1]\n",
    "\n",
    "# Apply the extraction to specified columns and find common numbers\n",
    "def process_column(tb_df, tf_df, column_name, target_count=10):\n",
    "    tb_df[[f'{column_name} Number', 'pdb_id']] = tb_df[column_name].apply(lambda x: pd.Series(extract_rank_and_pdb_id(x)))\n",
    "    tf_df[[f'{column_name} Number', 'pdb_id']] = tf_df[column_name].apply(lambda x: pd.Series(extract_rank_and_pdb_id(x)))\n",
    "\n",
    "    # Check for NaN values or flag values indicating conversion issues\n",
    "    tb_nan = tb_df[tb_df[[f'{column_name} Number', 'pdb_id']].isnull().any(axis=1)]\n",
    "    tf_nan = tf_df[tf_df[[f'{column_name} Number', 'pdb_id']].isnull().any(axis=1)]\n",
    "\n",
    "    tb_df.dropna(subset=[f'{column_name} Number', 'pdb_id'], inplace=True)\n",
    "    tf_df.dropna(subset=[f'{column_name} Number', 'pdb_id'], inplace=True)\n",
    "    \n",
    "    def find_multiple_similar_numbers(tb_df, tf_df, target_count=10):\n",
    "        window_size = 1\n",
    "        common_numbers_positions = []\n",
    "        \n",
    "        while len(common_numbers_positions) < target_count:\n",
    "            tb_top_ranks = tb_df.head(window_size)\n",
    "            tf_top_ranks = tf_df.head(window_size)\n",
    "            \n",
    "            # Find the intersection of the two series\n",
    "            common_numbers = set(tb_top_ranks[f'{column_name} Number']).intersection(set(tf_top_ranks[f'{column_name} Number']))\n",
    "            \n",
    "            for number in common_numbers:\n",
    "                if all(number != x[0] for x in common_numbers_positions):\n",
    "                    tb_row = tb_df[tb_df[f'{column_name} Number'] == number].iloc[0]\n",
    "                    tf_row = tf_df[tf_df[f'{column_name} Number'] == number].iloc[0]\n",
    "                    tb_position = tb_row.name + 1\n",
    "                    tf_position = tf_row.name + 1\n",
    "                    pdb_id = tb_row['pdb_id']\n",
    "                    common_numbers_positions.append((number, pdb_id, tb_position, tf_position))\n",
    "                    \n",
    "            window_size += 1\n",
    "            \n",
    "            # To avoid infinite loop in case there are not enough common numbers\n",
    "            if window_size > max(len(tb_df), len(tf_df)):\n",
    "                break\n",
    "\n",
    "        # Sort the common numbers based on the positions (TB position + TF position)\n",
    "        sorted_common_numbers_positions = sorted(common_numbers_positions, key=lambda x: x[2] + x[3])\n",
    "        \n",
    "        return sorted_common_numbers_positions, window_size\n",
    "\n",
    "    sorted_common_numbers_positions, window_size = find_multiple_similar_numbers(tb_df, tf_df, target_count)\n",
    "\n",
    "    # Combine the lists and sort by TB and TF positions\n",
    "    combined_list = []\n",
    "    for num, pdb_id, tb_pos, tf_pos in sorted_common_numbers_positions:\n",
    "        combined_list.append((f\"{num}_{pdb_id}_TB\", tb_pos))\n",
    "        combined_list.append((f\"{num}_{pdb_id}_TF\", tf_pos))\n",
    "\n",
    "    # Sort by position\n",
    "    sorted_combined_list = sorted(combined_list, key=lambda x: x[1])\n",
    "    \n",
    "    return pd.DataFrame(sorted_combined_list, columns=[f'{column_name}', 'Position']), tb_nan, tf_nan\n",
    "\n",
    "# Track files with NaN values\n",
    "files_with_nan = []\n",
    "\n",
    "# Process each file in the tb_dir\n",
    "for tb_file_name in os.listdir(tb_dir):\n",
    "    if tb_file_name.endswith('.xlsx'):\n",
    "        tb_file_path = os.path.join(tb_dir, tb_file_name)\n",
    "        \n",
    "        # Extract the PDB ID from the tb_file_name\n",
    "        pdb_id = tb_file_name.split('_')[0]\n",
    "        \n",
    "        # Construct the corresponding tf_file_name\n",
    "        tf_file_name = f'{pdb_id}_Ranked_TF.xlsx'\n",
    "        tf_file_path = os.path.join(tf_dir, tf_file_name)\n",
    "        \n",
    "        if os.path.exists(tf_file_path):\n",
    "            tb_df = pd.read_excel(tb_file_path)\n",
    "            tf_df = pd.read_excel(tf_file_path)\n",
    "            \n",
    "            columns_to_process = [\"AlphaFold Rank\", \"Fold_S Rank\", \"Hadd_md Rank\"]\n",
    "            results = []\n",
    "            nan_info = {'tb_file': tb_file_name, 'tf_file': tf_file_name, 'columns': {}}\n",
    "\n",
    "            for column in columns_to_process:\n",
    "                result_df, tb_nan, tf_nan = process_column(tb_df, tf_df, column, target_count=10)\n",
    "                results.append(result_df)\n",
    "                \n",
    "                if not tb_nan.empty or not tf_nan.empty:\n",
    "                    nan_info['columns'][column] = {'tb_nan': tb_nan, 'tf_nan': tf_nan}\n",
    "                    \n",
    "            # Save NaN information if there are any NaN values found\n",
    "            if nan_info['columns']:\n",
    "                files_with_nan.append(nan_info)\n",
    "\n",
    "            # Merge all results into one DataFrame\n",
    "            final_df = pd.concat(results, axis=1)\n",
    "\n",
    "            # Remove the 'Position' columns\n",
    "            position_columns = [col for col in final_df.columns if 'Position' in col]\n",
    "            final_df = final_df.drop(columns=position_columns)\n",
    "\n",
    "            # Save the final DataFrame to an Excel file\n",
    "            output_file_path = os.path.join(output_dir, f'{pdb_id}_combined.xlsx')\n",
    "            final_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "            #print(f\"Results saved to {output_file_path}\")\n",
    "\n",
    "# Print files with NaN values\n",
    "for nan_file in files_with_nan:\n",
    "    print(f\"File {nan_file['tb_file']} and {nan_file['tf_file']} have NaN values in the following columns:\")\n",
    "    for column, data in nan_file['columns'].items():\n",
    "        print(f\"  Column: {column}\")\n",
    "        print(f\"    TB NaN values:\\n{data['tb_nan']}\")\n",
    "        print(f\"    TF NaN values:\\n{data['tf_nan']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c815d-218a-4bfb-8bd8-6198d8b128d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the Excel files\n",
    "input_directory = 'path to/Merged/combined_TB_TF_md'\n",
    "output_directory = 'output to/Merged/combined_TB_TF_top20_md'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process each Excel file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Load the Excel file\n",
    "        excel_data = pd.ExcelFile(file_path)\n",
    "        \n",
    "        # Assume the sheet we need is the first one\n",
    "        sheet_name = excel_data.sheet_names[0]\n",
    "        \n",
    "        # Load the data from the sheet\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        \n",
    "        # Keep only the top 10 rows\n",
    "        df_top_10 = df.head(20)\n",
    "        \n",
    "        # Save the modified dataframe to a new Excel file in the output directory\n",
    "        output_path = os.path.join(output_directory, filename)\n",
    "        df_top_10.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"Processing complete. Files saved to\", output_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d352e-6022-4471-8129-ed9ac2da895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib_venn import venn3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path to the directory containing the Excel files\n",
    "directory_path = 'path to/Merged/combined_TB_TF_top20_md'\n",
    "\n",
    "# Define the scoring function column names\n",
    "scoring_functions = ['AlphaFold Rank', 'Fold_S Rank', 'Hadd_md Rank']\n",
    "\n",
    "# Initialize a dictionary to store models for each scoring function\n",
    "all_models = {func: set() for func in scoring_functions}\n",
    "\n",
    "# Function to add models to respective sets\n",
    "def merge_models(df):\n",
    "    for func in scoring_functions:\n",
    "        all_models[func].update(df[func].dropna().unique())\n",
    "\n",
    "# Iterate over all Excel files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        df = pd.read_excel(file_path)\n",
    "        merge_models(df)\n",
    "\n",
    "# Calculate common models among all three scoring functions\n",
    "common_all_three = set.intersection(*[all_models[func] for func in scoring_functions])\n",
    "\n",
    "# Plot the Venn diagram\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn3([all_models[func] for func in scoring_functions], set_labels=scoring_functions)\n",
    "plt.title('Common Models Among All Scoring Functions')\n",
    "plt.show()\n",
    "\n",
    "# Print the common models\n",
    "print(\"Common models among all three scoring functions:\")\n",
    "print(common_all_three)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ecc7b-c7e2-4d59-82ed-656d0f52a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the common models file\n",
    "common_models_path = 'path to/Merged/Output_TB_TF/common_models_md_20.xlsx'\n",
    "common_models_df = pd.read_excel(common_models_path)\n",
    "\n",
    "# Print the first few rows of the DataFrame to check its structure\n",
    "print(\"Common models DataFrame head:\")\n",
    "print(common_models_df.head())\n",
    "\n",
    "# Check the number of columns in the DataFrame\n",
    "print(\"Number of columns in the common models DataFrame:\", len(common_models_df.columns))\n",
    "\n",
    "# Add a new column for Combined DockQ at index 1\n",
    "common_models_df.insert(1, 'Combined DockQ', '')\n",
    "\n",
    "# Define the directories for TB and TF files\n",
    "tb_directory = 'path to /AlphaFold_Multimer_All_TB'\n",
    "tf_directory = 'path to/AlphaFold_Multimer_All_TF'\n",
    "\n",
    "# Function to get DockQ value from the corresponding file\n",
    "def get_dockq_value(model_name):\n",
    "    parts = model_name.split('_')\n",
    "    first_number = parts[0]\n",
    "    pdb_id = parts[1]\n",
    "    file_type = parts[-1]\n",
    "    \n",
    "    directory = tb_directory if file_type == 'TB' else tf_directory\n",
    "    file_path = os.path.join(directory, f'{pdb_id}_DockQ_data_{file_type}.xlsx')\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        dockq_df = pd.read_excel(file_path)\n",
    "        file_name = f'ranked_{first_number}.pdb_clean'\n",
    "        dockq_value = dockq_df.loc[dockq_df['File Name'] == file_name, 'DockQ'].values\n",
    "        if len(dockq_value) > 0:\n",
    "            return dockq_value[0]\n",
    "    return None\n",
    "\n",
    "# Extract DockQ values for each model in the common models file\n",
    "common_models_df['Combined DockQ'] = common_models_df['File name'].apply(get_dockq_value)\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file_path = 'output to/Merged/Output_TB_TF/updated_common_models_md_20.xlsx'\n",
    "common_models_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated common models file has been saved to {output_file_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (evaluation)",
   "language": "python",
   "name": "evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
